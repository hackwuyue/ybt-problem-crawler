# YBT é¢˜ç›®çˆ¬è™«å·¥å…· - å®ç°è¯¦è§£

## ğŸ“‹ ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [æ ¸å¿ƒåŠŸèƒ½](#æ ¸å¿ƒåŠŸèƒ½)
3. [æŠ€æœ¯æ¶æ„](#æŠ€æœ¯æ¶æ„)
4. [å®ç°ç»†èŠ‚](#å®ç°ç»†èŠ‚)
5. [API å‚è€ƒ](#api-å‚è€ƒ)
6. [æ‰©å±•ä¸å®šåˆ¶](#æ‰©å±•ä¸å®šåˆ¶)

---

## é¡¹ç›®æ¦‚è¿°

### é¡¹ç›®åç§°
**YBT é¢˜ç›®çˆ¬è™«å·¥å…·** (YBT Problem Crawler)

### é¡¹ç›®æè¿°
ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„ Python çˆ¬è™«æ¡†æ¶ï¼Œä¸“ç”¨äºæŠ“å–ä¿¡æ¯å­¦å¥¥èµ›ä¸€æœ¬é€šï¼ˆYBTï¼‰åœ¨çº¿è¯„æµ‹ç³»ç»Ÿçš„é¢˜ç›®ä¿¡æ¯ã€æ ·ä¾‹å’Œå›¾ç‰‡ï¼Œå¹¶ç”Ÿæˆæ ‡å‡† SQL å¯¼å…¥æ–‡ä»¶ã€‚

### å…³é”®ç‰¹æ€§
- âœ… **HTTP å¥å£®æ€§**ï¼šè‡ªåŠ¨é‡è¯•ã€è¶…æ—¶å¤„ç†ã€é™é€Ÿæ¢å¤
- âœ… **å¹¶å‘çˆ¬å–**ï¼šçº¿ç¨‹æ± å¹¶å‘ï¼Œæ”¯æŒè‡ªå®šä¹‰ worker æ•°é‡
- âœ… **æ–­ç‚¹ç»­çˆ¬**ï¼šåŸºäº JSON å¿«ç…§ï¼Œç½‘ç»œä¸­æ–­åè‡ªåŠ¨æ¢å¤
- âœ… **çµæ´»è¾“å‡º**ï¼šæ”¯æŒ JSON/SQL/çº¯æ–‡æœ¬ç­‰å¤šç§æ ¼å¼
- âœ… **å®Œæ•´ CLI**ï¼šargparse æ¥å£ï¼Œæ”¯æŒå¤æ‚å‘½ä»¤ç»„åˆ
- âœ… **æ—¥å¿—è¿½è¸ª**ï¼šè¯¦ç»†çš„è¿è¡Œæ—¥å¿—ï¼Œä¾¿äºé—®é¢˜æ’æŸ¥

### åº”ç”¨åœºæ™¯
- å°† YBT é¢˜åº“æ‰¹é‡å¯¼å…¥æœ¬åœ°æ•°æ®åº“
- ä¸º OJ ç³»ç»Ÿå‡†å¤‡é¢˜ç›®æ•°æ®
- é¢˜ç›®ä¿¡æ¯åˆ†æä¸ç»Ÿè®¡
- é¢˜ç›®å¤‡ä»½ä¸å½’æ¡£

---

## æ ¸å¿ƒåŠŸèƒ½

### 1. åŸºç¡€çˆ¬å–

**åŠŸèƒ½æè¿°**ï¼šçˆ¬å–æŒ‡å®šèŒƒå›´å†…çš„é¢˜ç›®ä¿¡æ¯ã€‚

```bash
python yibentong.py 1000 1010
```

**äº§å‡º**ï¼š
- `problems_1000_1010.json` - é¢˜ç›®æ•°æ®å¿«ç…§
- `problems_1000_1010.sql` - SQL å¯¼å…¥æ–‡ä»¶
- `data/{pid}/sample.in` å’Œ `sample.out` - æ ·ä¾‹æ–‡ä»¶
- `image/{pid}/*.png/.jpg` - é¢˜ç›®å›¾ç‰‡

**è€—æ—¶**ï¼šçº¦ 10 ç§’ï¼ˆ11 é¢˜ï¼‰

### 2. å¹¶å‘çˆ¬å–

**åŠŸèƒ½æè¿°**ï¼šä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘çˆ¬å–ï¼Œå¤§å¹…æå‡é€Ÿåº¦ã€‚

```bash
python yibentong.py 1000 1010 --concurrent 4
```

**æ€§èƒ½æå‡**ï¼š3-4 å€åŠ é€Ÿï¼ˆå•çº¿ç¨‹ 10s -> å¹¶å‘ 2.5sï¼‰

**worker æ•°é‡å»ºè®®**ï¼š
- 2ï¼šç¨³å®šï¼Œæ— é™é€Ÿé£é™©
- 3ï¼šå¹³è¡¡ï¼ˆæ¨èï¼‰
- 4ï¼šæ¿€è¿›ï¼Œå¯èƒ½è§¦å‘é™é€Ÿ

### 3. æ–­ç‚¹ç»­çˆ¬

**åŠŸèƒ½æè¿°**ï¼šç½‘ç»œä¸­æ–­åï¼Œè‡ªåŠ¨è¯†åˆ«å·²å®Œæˆçš„é¢˜ç›®ï¼Œä»…çˆ¬å–ç¼ºå¤±éƒ¨åˆ†ã€‚

```bash
# ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆä¸­æ–­ï¼‰
python yibentong.py 1000 1010 --concurrent 4

# ç½‘ç»œæ¢å¤åé‡æ–°è¿è¡Œ
python yibentong.py 1000 1010 --resume --concurrent 4
```

**å·¥ä½œåŸç†**ï¼š
1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨ `problems_1000_1010.json`
2. å¦‚æœå­˜åœ¨ï¼Œè¯»å–å·²å®Œæˆçš„é¢˜ç›® ID
3. ä»…çˆ¬å–ç¼ºå¤±çš„é¢˜ç›®
4. åˆå¹¶ç»“æœåé‡æ–°ç”Ÿæˆ SQL

**ä¼˜åŠ¿**ï¼š
- çœæ—¶ï¼šé¿å…é‡å¤çˆ¬å–
- çœç½‘ï¼šä»…ä¼ è¾“ç¼ºå¤±æ•°æ®
- å®‰å…¨ï¼šæ— éœ€æ‹…å¿ƒä¸­æ–­

### 4. JSON-only æ¨¡å¼

**åŠŸèƒ½æè¿°**ï¼šåŸºäºç°æœ‰ JSON æ–‡ä»¶é‡æ–°ç”Ÿæˆ SQLï¼Œæ— éœ€ç½‘ç»œè¯·æ±‚ã€‚

```bash
# ä¿®æ”¹ SQL ç”Ÿæˆè§„åˆ™å
python yibentong.py 1000 1010 --json-only
```

**ä½¿ç”¨åœºæ™¯**ï¼š
- ä¿®æ”¹ SQL ç”Ÿæˆé€»è¾‘ï¼ˆå¦‚æ·»åŠ æ–°å­—æ®µï¼‰
- ä¿®æ”¹æ•°æ®åº“å­—ç¬¦ç¼–ç é…ç½®
- å¿«é€Ÿé‡æ–°ç”Ÿæˆ SQLï¼ˆè€—æ—¶ <1 ç§’ï¼‰

### 5. è·³è¿‡å›¾ç‰‡ä¸‹è½½

**åŠŸèƒ½æè¿°**ï¼šä»…çˆ¬å–é¢˜ç›®æ–‡æœ¬ä¿¡æ¯ï¼Œè·³è¿‡å›¾ç‰‡ä¸‹è½½ä»¥åŠ é€Ÿã€‚

```bash
python yibentong.py 1000 1010 --no-image
```

**åŠ é€Ÿæ•ˆæœ**ï¼šè€—æ—¶å‡å°‘ 60-70%ï¼ˆæœ‰å¤§é‡å›¾ç‰‡çš„æƒ…å†µä¸‹ï¼‰

**å…¸å‹ç”¨é€”**ï¼š
- å¿«é€Ÿè·å–æ‰€æœ‰é¢˜ç›®ä¿¡æ¯
- ç¨åå•ç‹¬è¡¥å……ä¸‹è½½å›¾ç‰‡
- ç½‘ç»œå¸¦å®½æœ‰é™çš„ç¯å¢ƒ

### 6. é€Ÿç‡æ§åˆ¶

**åŠŸèƒ½æè¿°**ï¼šè°ƒæ•´è¯·æ±‚é—´éš”ï¼Œå¹³è¡¡é€Ÿåº¦ä¸æœåŠ¡å™¨å‹åŠ›ã€‚

```bash
# é»˜è®¤å»¶è¿Ÿ 0.5 ç§’
python yibentong.py 1000 1010

# åŠ å¿«é€Ÿåº¦ï¼ˆå»¶è¿Ÿ 0.1 ç§’ï¼‰
python yibentong.py 1000 1010 --rate 0.1

# é™ä½å‹åŠ›ï¼ˆå»¶è¿Ÿ 1.0 ç§’ï¼‰
python yibentong.py 1000 1010 --rate 1.0
```

**å»ºè®®å€¼**ï¼š
- 0.05 ç§’ï¼šæé™åŠ é€Ÿï¼ˆæœ‰è¢«é™é€Ÿé£é™©ï¼‰
- 0.1 ç§’ï¼šå¿«é€Ÿçˆ¬å–ï¼ˆæ¨èç”¨äºå¹¶å‘ï¼‰
- 0.5 ç§’ï¼šé»˜è®¤ï¼ˆå¹³è¡¡ï¼‰
- 1.0+ ç§’ï¼šä¿å®ˆï¼ˆé™ä½æœåŠ¡å™¨å‹åŠ›ï¼‰

---

## æŠ€æœ¯æ¶æ„

### æ•´ä½“è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Command Line Interface              â”‚
â”‚         (argparse å‚æ•°è§£æ)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ä¸»å‡½æ•° (main)                        â”‚
â”‚  â€¢ å‚æ•°éªŒè¯                                   â”‚
â”‚  â€¢ å·¥ä½œæµç¨‹æ§åˆ¶                               â”‚
â”‚  â€¢ å…¨å±€æ ‡å¿—è®¾ç½®                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚
         â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ å•çº¿ç¨‹   â”‚    â”‚ å¤šçº¿ç¨‹   â”‚
    â”‚çˆ¬å–æ¨¡å¼  â”‚    â”‚å¹¶å‘æ¨¡å¼  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚              â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  çˆ¬å–æ ¸å¿ƒå‡½æ•°       â”‚
         â”‚  crawl_problem()    â”‚
         â”‚  â€¢ è¯·æ±‚ç½‘é¡µ          â”‚
         â”‚  â€¢ è§£æå†…å®¹          â”‚
         â”‚  â€¢ ä¸‹è½½å›¾ç‰‡          â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                 â”‚
         â–¼                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ JSON ç”Ÿæˆ  â”‚   â”‚HTML å¤„ç† â”‚
    â”‚ä¿å­˜å¿«ç…§    â”‚   â”‚å›¾ç‰‡ä¸‹è½½  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
             â”‚            â”‚
             â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ SQL ç”Ÿæˆ     â”‚
           â”‚è§„èŒƒåŒ–å›¾ç‰‡å¼•ç”¨ â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒæ¨¡å—

#### 1. HTTP ä¼šè¯ç®¡ç† (`make_session`)

```python
def make_session():
    """åˆ›å»ºå¸¦é‡è¯•ç­–ç•¥çš„ Session"""
    session = requests.Session()
    retry = Retry(
        total=5,                        # æœ€å¤šé‡è¯• 5 æ¬¡
        backoff_factor=0.6,             # é€€é¿å› å­
        status_forcelist=[429,500,502,503,504]  # å¯é‡è¯•çš„çŠ¶æ€ç 
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session
```

**é‡è¯•ç­–ç•¥**ï¼š
- è¿æ¥è¢«æ‹’ç» â†’ ç«‹å³é‡è¯•
- 429 (Too Many Requests) â†’ ç­‰å¾…åé‡è¯•
- 5xx (æœåŠ¡å™¨é”™è¯¯) â†’ æŒ‡æ•°é€€é¿é‡è¯•
- æˆåŠŸå“åº” â†’ ç›´æ¥è¿”å›

#### 2. å›¾ç‰‡å¤„ç† (`process_images_in_html`)

```python
def process_images_in_html(soup, problem_id, page_url, session):
    """ä¸‹è½½å¹¶å¤„ç† HTML ä¸­çš„å›¾ç‰‡"""
    if SKIP_IMAGES:  # å…¨å±€æ ‡å¿—ï¼šè·³è¿‡å›¾ç‰‡
        return soup
    
    # éå†æ‰€æœ‰ <img> æ ‡ç­¾
    for idx, img_tag in enumerate(img_tags, start=1):
        src = img_tag.get('src')
        img_url = src if src.startswith('http') else urljoin(page_url, src)
        
        # æµå¼ä¸‹è½½ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰
        with session.get(img_url, stream=True) as img_response:
            # ç¡®å®šæ–‡ä»¶æ ¼å¼
            ext = infer_extension(img_response.headers)
            
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åï¼š{pid}_{idx}_{md5}.{ext}
            h = hashlib.md5(img_url.encode()).hexdigest()[:10]
            filename = f"{pid}_{idx}_{h}{ext}"
            
            # åˆ†å—å†™å…¥æ–‡ä»¶
            with open(filepath, 'wb') as f:
                for chunk in img_response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # åˆ›å»ºå…¼å®¹å‰¯æœ¬ï¼š{pid}.{ext}
            if not os.path.exists(compat_path):
                shutil.copy(filepath, compat_path)
```

**å…³é”®ç‰¹æ€§**ï¼š
- **æµå¼ä¸‹è½½**ï¼š`stream=True` + `iter_content()`ï¼Œé¿å…å¤§æ–‡ä»¶ä¸€æ¬¡æ€§åŠ è½½
- **å”¯ä¸€å‘½å**ï¼šä½¿ç”¨ MD5 å“ˆå¸Œï¼Œé¿å…å†²çª
- **å…¼å®¹å‰¯æœ¬**ï¼šä¸ºæ—§ç³»ç»Ÿåˆ›å»ºç®€å•çš„ `{pid}.{ext}` å‰¯æœ¬

#### 3. å¹¶å‘çˆ¬å– (`crawl_ids_concurrent`)

```python
def crawl_ids_concurrent(id_list, max_workers=3, rate_delay=0.5):
    """ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘çˆ¬å–"""
    results = {}
    failed = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(crawl_problem, pid, make_session()): pid 
            for pid in id_list
        }
        
        for future in as_completed(futures):
            pid = futures[future]
            try:
                data = future.result()
                if data:
                    results[pid] = data
                else:
                    failed.append(pid)
            except Exception as e:
                logging.exception(f"çˆ¬å– {pid} å¤±è´¥: {e}")
                failed.append(pid)
    
    return results, failed
```

**è®¾è®¡è¦ç‚¹**ï¼š
- æ¯ä¸ª worker åˆ›å»ºç‹¬ç«‹ Sessionï¼Œé¿å…çº¿ç¨‹é—´ç«äº‰
- ä½¿ç”¨ `as_completed()` è·å¾—ç»“æœï¼Œæé«˜å“åº”æ€§
- è‡ªåŠ¨æ•è·å¼‚å¸¸ï¼Œé˜²æ­¢å•ä¸ªå¤±è´¥å½±å“æ•´ä½“

#### 4. SQL ç”Ÿæˆ (`generate_sql_insert`)

```python
def generate_sql_insert(problem_data, problem_id):
    """ç”Ÿæˆå•æ¡ SQL INSERT è¯­å¥"""
    
    # è§„èŒƒåŒ–å›¾ç‰‡å¼•ç”¨
    description = replace_image_srcs(problem_data['description'], problem_id)
    
    # SQL è½¬ä¹‰
    title = escape_sql(problem_data['title'])
    description = escape_sql(description)
    
    # å•ä½è½¬æ¢
    time_limit = float(problem_data['time_limit']) / 1000  # ms -> s
    memory_limit = int(problem_data['memory_limit']) // 1024  # KB -> MB
    
    # ç”Ÿæˆ SQL
    sql = f"""INSERT INTO `problem` (
        `title`, `description`, `input`, `output`, `sample_input`, `sample_output`,
        `spj`, `hint`, `source`, `in_date`, `time_limit`, `memory_limit`,
        `defunct`, `accepted`, `submit`, `solved`, `remote_oj`, `remote_id`
    ) VALUES (
        '{title}',
        '{description}',
        ...
        {time_limit:.3f},
        {memory_limit},
        ...
    );"""
    
    return sql
```

**å…³é”®æ­¥éª¤**ï¼š
1. è§„èŒƒåŒ–å›¾ç‰‡å¼•ç”¨ï¼ˆç”¨å…¼å®¹ä¸»åæ›¿æ¢ hash ç‰ˆæœ¬ï¼‰
2. SQL è½¬ä¹‰ï¼ˆå¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
3. å•ä½è½¬æ¢ï¼ˆmsâ†’s, KBâ†’MBï¼‰
4. ç”Ÿæˆæ ‡å‡† SQL INSERT è¯­å¥

#### 5. æ–­ç‚¹ç»­çˆ¬ (`resume` é€»è¾‘)

```python
# è¯†åˆ«å·²å®Œæˆçš„é¢˜ç›®
completed = set()
if args.resume and os.path.exists(json_file):
    with open(json_file) as jf:
        prev = json.load(jf)
    completed = {int(k) for k in prev.keys()}
    logging.info(f'ä» JSON æ¢å¤ï¼Œå·²è·³è¿‡ {len(completed)} é¡¹')

# è®¡ç®—ç¼ºå¤±çš„é¢˜ç›®
ids = [pid for pid in range(start_id, end_id+1) if pid not in completed]

# å¦‚æœæ— ç¼ºå¤±é¢˜ç›®ï¼Œç›´æ¥åŸºäº JSON ç”Ÿæˆ SQL
if not ids:
    create_sql_file(existing_problems, start_id, end_id)
    return
```

---

## å®ç°ç»†èŠ‚

### é¢˜ç›®å­˜åœ¨æ€§æ£€æµ‹

**åŠŸèƒ½æè¿°**ï¼šè‡ªåŠ¨æ£€æµ‹é¢˜ç›®æ˜¯å¦å­˜åœ¨ï¼Œé¿å…ä¸ºä¸å­˜åœ¨çš„é¢˜ç›®åˆ›å»ºå†—ä½™æ•°æ®ã€‚

**æ£€æµ‹æœºåˆ¶**ï¼š

1. **åœ¨çˆ¬å–é˜¶æ®µæ ‡è®°**

```python
def crawl_problem(problem_id, session):
    # æŸ¥æ‰¾é¢˜ç›®æ ‡é¢˜
    for h3 in soup.find_all('h3'):
        h3_text = h3.get_text(strip=True)
        if str(problem_id) in h3_text:
            title = h3_text
            problem_exists = True
            break
    
    # å¦‚æœæœªæ‰¾åˆ°æ ‡é¢˜ï¼Œæ ‡è®°ä¸ºä¸å­˜åœ¨
    if not title:
        title = f"{problem_id}ï¼šæœªçŸ¥é¢˜ç›®"
        problem_exists = False
    
    # åœ¨è¿”å›æ•°æ®ä¸­æ·»åŠ æ ‡è®°
    pdata['exists'] = problem_exists
    return pdata
```

2. **åœ¨ä¿å­˜é˜¶æ®µæ£€æŸ¥**

```python
def save_sample_files(problem_data, problem_id):
    if not problem_data.get('exists', True):
        logging.info('è·³è¿‡ä¸å­˜åœ¨çš„é¢˜ç›®: %s', problem_id)
        return False
    # æ­£å¸¸ä¿å­˜æ–‡ä»¶...
```

3. **åœ¨ SQL ç”Ÿæˆé˜¶æ®µæ£€æŸ¥**

```python
def generate_sql_insert(problem_data, problem_id):
    if not problem_data.get('exists', True):
        logging.info('è·³è¿‡ä¸å­˜åœ¨çš„é¢˜ç›®SQLç”Ÿæˆ: %s', problem_id)
        return None
    # æ­£å¸¸ç”Ÿæˆ SQL...
```

**æ•ˆæœ**ï¼š
- âœ… ä¸åˆ›å»ºä¸å­˜åœ¨é¢˜ç›®çš„æ–‡ä»¶å¤¹
- âœ… ä¸ç”Ÿæˆä¸å­˜åœ¨é¢˜ç›®çš„ SQL INSERT è¯­å¥
- âœ… JSON ä¸­ä¿ç•™ `"exists": false` æ ‡è®°ç”¨äºè¿½è¸ª

**ç¤ºä¾‹**ï¼š
```json
{
  "2228": {
    "title": "2228ï¼šæœªçŸ¥é¢˜ç›®",
    "exists": false,
    "description": "",
    "input": "",
    "output": ""
  }
}
```

### HTML å†…å®¹æå–

**åŸå§‹ HTML ç»“æ„**ï¼š

```html
<script>
pshow("&lt;p&gt;è¿™æ˜¯é¢˜ç›®æè¿°&lt;/p&gt;&lt;img src='...'&gt;")
</script>
```

**æå–æ­¥éª¤**ï¼š

1. **åŒ¹é… pshow å‡½æ•°**

```python
match = re.search(r'pshow\("(.*?)"\)', script_content, re.DOTALL)
content = match.group(1)
```

2. **æ¸…ç†è½¬ä¹‰å­—ç¬¦**

```python
content = clean_html_content(content)
content = content.replace('\\n', '\n').replace('\\t', '\t')
```

3. **è§£æä¸º BeautifulSoup**

```python
soup = BeautifulSoup(content, 'html.parser')
soup = process_images_in_html(soup, problem_id, page_url, session)
```

4. **è·å–æœ€ç»ˆ HTML**

```python
html_output = str(soup)
```

### å›¾ç‰‡å¼•ç”¨è§„èŒƒåŒ–

**é—®é¢˜**ï¼šSQL ä¸­çš„å›¾ç‰‡è·¯å¾„ä¸å®é™…æ–‡ä»¶ä¸åŒ¹é…

```
åŸå§‹å¼•ç”¨:  /upload/image/1445/xxxx_hash.png
å®é™…æ–‡ä»¶:  image/1445/1445.png
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
def replace_image_srcs(html_content, pid):
    """å°† HTML ä¸­çš„å›¾ç‰‡å¼•ç”¨æ›¿æ¢ä¸ºå…¼å®¹ä¸»å"""
    # æŸ¥æ‰¾ä¸»åå‰¯æœ¬ (e.g., 1445.png)
    primary = find_primary_image_filename(pid)
    if not primary:
        return html_content
    
    # å°†æ‰€æœ‰ /upload/image/{pid}/xxxx æ›¿æ¢ä¸º /upload/image/{pid}/primary
    pattern = re.compile(rf'(/upload/image/{pid}/)[^"\'>\s]+')
    return pattern.sub(rf'\1{primary}', html_content)
```

**æ‰§è¡Œæ—¶æœº**ï¼šåœ¨ç”Ÿæˆ SQL å‰å¯¹ JSON è¿›è¡Œè§„èŒƒåŒ–å¤„ç†

### å¹¶å‘æ§åˆ¶

**å»¶è¿Ÿæœºåˆ¶**ï¼š

```python
def crawl_worker(problem_id, rate_delay=0.5):
    """Worker å‡½æ•°"""
    session = make_session()
    try:
        data = crawl_problem(problem_id, session)
        time.sleep(rate_delay)  # é™ä½ç¬æ—¶å‹åŠ›
        return problem_id, data
    finally:
        session.close()
```

**æ•ˆæœ**ï¼šå³ä½¿ 4 ä¸ª worker å¹¶å‘è¿è¡Œï¼Œå®é™…è¯·æ±‚ä»æœ‰é€‚å½“é—´éš”ï¼Œé¿å… DoS é£é™©

### å…¨å±€æ ‡å¿—æ§åˆ¶

```python
# å…¨å±€å˜é‡
SKIP_IMAGES = False
JSON_ONLY = False

# åœ¨ main() ä¸­è®¾ç½®
SKIP_IMAGES = bool(args.no_image)
JSON_ONLY = bool(args.json_only)

# åœ¨çˆ¬å–é€»è¾‘ä¸­æ£€æŸ¥
if SKIP_IMAGES:
    return soup  # è·³è¿‡å›¾ç‰‡å¤„ç†
```

**ä¼˜åŠ¿**ï¼šé¿å…å¤šæ¬¡æ£€æŸ¥å‚æ•°ï¼Œæé«˜æ€§èƒ½

---

## API å‚è€ƒ

### å‘½ä»¤è¡Œæ¥å£

```bash
usage: yibentong.py [-h] [--concurrent N] [--resume] [--json-only] 
                    [--no-image] [--rate FLOAT] 
                    [start] [end]
```

### å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|------|--------|------|
| `start` | int | 1445 | èµ·å§‹é¢˜ç›® ID |
| `end` | int | 1445 | ç»“æŸé¢˜ç›® ID |
| `--concurrent` / `-c` | int | æ—  | å¹¶å‘ worker æ•°ï¼ˆ2-4 æ¨èï¼‰ |
| `--resume` | flag | False | å¯ç”¨æ–­ç‚¹ç»­çˆ¬ |
| `--json-only` | flag | False | ä»…åŸºäº JSON ç”Ÿæˆ SQL |
| `--no-image` | flag | False | è·³è¿‡å›¾ç‰‡ä¸‹è½½ |
| `--rate` | float | 0.5 | è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰ |

### ä½¿ç”¨ç¤ºä¾‹

```bash
# åŸºç¡€çˆ¬å–
python yibentong.py 1000 1010

# å¹¶å‘çˆ¬å–ï¼ˆ4 workerï¼‰
python yibentong.py 1000 1010 --concurrent 4

# å¹¶å‘ + æ–­ç‚¹ç»­çˆ¬
python yibentong.py 1000 1010 --concurrent 4 --resume

# å¿«é€Ÿçˆ¬å–ï¼ˆæ— å›¾ç‰‡ï¼‰
python yibentong.py 1000 1010 --no-image

# æé™åŠ é€Ÿï¼ˆå¹¶å‘ + æ— å›¾ç‰‡ + é«˜é€Ÿç‡ï¼‰
python yibentong.py 1000 1010 --concurrent 4 --no-image --rate 0.1

# ä»…åŸºäº JSON ç”Ÿæˆ SQL
python yibentong.py 1000 1010 --json-only
```

### è¿”å›å€¼ä¸è¾“å‡º

**JSON æ ¼å¼** (`problems_{start}_{end}.json`)ï¼š

```json
{
  "1000": {
    "title": "1000ï¼šæŸé¢˜åç§°",
    "description": "<p>é¢˜ç›®æè¿° HTML</p>",
    "input": "<p>è¾“å…¥æè¿°</p>",
    "output": "<p>è¾“å‡ºæè¿°</p>",
    "sample_input": "è¾“å…¥æ ·ä¾‹",
    "sample_output": "è¾“å‡ºæ ·ä¾‹",
    "time_limit": "1000",
    "memory_limit": "32768"
  },
  ...
}
```

**SQL æ ¼å¼** (`problems_{start}_{end}.sql`)ï¼š

```sql
INSERT INTO `problem` (
    `title`, `description`, `input`, `output`, `sample_input`, `sample_output`,
    `spj`, `hint`, `source`, `in_date`, `time_limit`, `memory_limit`,
    `defunct`, `accepted`, `submit`, `solved`, `remote_oj`, `remote_id`
) VALUES (
    '1000ï¼šæŸé¢˜åç§°',
    '<p>é¢˜ç›®æè¿° HTML</p>',
    ...
    1.000,
    32,
    ...
);
```

---

## æ‰©å±•ä¸å®šåˆ¶

### ä¿®æ”¹ SQL è¾“å‡ºå­—æ®µ

ç¼–è¾‘ `generate_sql_insert()` å‡½æ•°ï¼š

```python
def generate_sql_insert(problem_data, problem_id):
    # ... ç°æœ‰ä»£ç  ...
    
    # æ·»åŠ è‡ªå®šä¹‰å­—æ®µ
    difficulty = problem_data.get('difficulty', 'MEDIUM')
    
    sql = f"""INSERT INTO `problem` (
        ...,
        `difficulty`
    ) VALUES (
        ...,
        '{difficulty}'
    );"""
    
    return sql
```

### æ·»åŠ æ–°çš„å…ƒæ•°æ®æå–

ä¿®æ”¹ `crawl_problem()` å‡½æ•°ï¼š

```python
def crawl_problem(problem_id, session):
    # ... ç°æœ‰ä»£ç  ...
    
    # æ–°å¢ï¼šæå–ä½œè€…ä¿¡æ¯
    author = extract_author_info(soup)
    
    problem_data = {
        # ... ç°æœ‰å­—æ®µ ...
        'author': author
    }
    
    return problem_data
```

### è‡ªå®šä¹‰å›¾ç‰‡å¤„ç†

åœ¨ `process_images_in_html()` ä¸­æ·»åŠ ï¼š

```python
def process_images_in_html(soup, problem_id, page_url, session):
    if SKIP_IMAGES:
        return soup
    
    for idx, img_tag in enumerate(img_tags, start=1):
        # ... ç°æœ‰ä¸‹è½½é€»è¾‘ ...
        
        # è‡ªå®šä¹‰ï¼šå‹ç¼©å›¾ç‰‡
        compress_image(filepath)
        
        # è‡ªå®šä¹‰ï¼šç”Ÿæˆç¼©ç•¥å›¾
        generate_thumbnail(filepath)
    
    return soup
```

### é›†æˆåˆ° OJ ç³»ç»Ÿ

```python
# ç›´æ¥å¯¼å…¥åº“
from yibentong import crawl_problem, create_sql_file

# ä½¿ç”¨çˆ¬è™« API
problems = crawl_problem(1000, session)
sql = generate_sql_insert(problems, 1000)

# æ’å…¥æ•°æ®åº“
db.execute(sql)
```

---

## å¸¸è§é—®é¢˜ä¸è°ƒè¯•

### Q: çˆ¬å–é€Ÿåº¦å¾ˆæ…¢ï¼Ÿ

**A: å°è¯•ä»¥ä¸‹æ–¹æ¡ˆï¼š**

1. å¢åŠ å¹¶å‘æ•°

```bash
python yibentong.py 1000 1010 --concurrent 4
```

2. é™ä½å»¶è¿Ÿ

```bash
python yibentong.py 1000 1010 --rate 0.1
```

3. è·³è¿‡å›¾ç‰‡

```bash
python yibentong.py 1000 1010 --no-image
```

4. ç»„åˆæ–¹æ¡ˆ

```bash
python yibentong.py 1000 1010 --concurrent 4 --no-image --rate 0.1
```

### Q: æŸäº›é¢˜ç›®çˆ¬å–å¤±è´¥ï¼Ÿ

**A: æ£€æŸ¥æ—¥å¿—å¹¶é‡è¯•ï¼š**

```bash
# æŸ¥çœ‹é”™è¯¯ä¿¡æ¯
tail -f crawler.log | grep ERROR

# ä½¿ç”¨ --resume é‡æ–°å°è¯•
python yibentong.py 1000 1010 --resume
```

### Q: å›¾ç‰‡å¼•ç”¨ä¸æ­£ç¡®ï¼Ÿ

**A: æ£€æŸ¥å…¼å®¹å‰¯æœ¬ï¼š**

```bash
# éªŒè¯å›¾ç‰‡æ–‡ä»¶
ls -la image/1000/

# åº”è¯¥æœ‰ 1000_1_<hash>.png å’Œ 1000.png ä¸¤ä¸ªæ–‡ä»¶
```

å¦‚æœç¼ºå°‘å…¼å®¹å‰¯æœ¬ï¼Œåˆ é™¤ SQL åé‡æ–°ç”Ÿæˆï¼š

```bash
rm problems_1000_1010.sql
python yibentong.py 1000 1010 --json-only
```

### Q: å¦‚ä½•ä¿®æ”¹æ•°æ®åº“å¯¼å…¥é…ç½®ï¼Ÿ

**A: ç¼–è¾‘ `create_sql_file()` ä¸­çš„è¡¨ç»“æ„ï¼š**

```python
def create_sql_file(problems, start_id, end_id):
    sql_content = """
    CREATE TABLE IF NOT EXISTS `problem` (
      ...
      `your_field` VARCHAR(255),  # æ·»åŠ è‡ªå®šä¹‰å­—æ®µ
      ...
    );
    """
```

---

## æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. è°ƒæ•´çº¿ç¨‹æ± å¤§å°

```python
# é»˜è®¤ 3 workerï¼Œå¯æ ¹æ®ç½‘ç»œçŠ¶å†µè°ƒæ•´
python yibentong.py 1000 2000 --concurrent 2  # ç¨³å®š
python yibentong.py 1000 2000 --concurrent 4  # æ¿€è¿›
```

### 2. åˆ†é˜¶æ®µçˆ¬å–

```bash
# ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€Ÿè·å–æ–‡æœ¬ä¿¡æ¯
python yibentong.py 1000 2000 --concurrent 4 --no-image --rate 0.1

# ç¬¬äºŒé˜¶æ®µï¼šè¡¥å……ä¸‹è½½å›¾ç‰‡
python yibentong.py 1000 2000 --resume
```

### 3. ç›‘æ§æ—¥å¿—

```bash
# å®æ—¶æŸ¥çœ‹çˆ¬å–è¿›åº¦
tail -f crawler.log | grep "å·²çˆ¬å–\|å·²ä¸‹è½½"
```

---

**æ›´æ–°æ—¥æœŸ**ï¼š2026-01-13  
**ç‰ˆæœ¬**ï¼š1.0 (ç”Ÿäº§å°±ç»ª)
